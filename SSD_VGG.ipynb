{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are just a student and dont have enough computation power and/or training set  \n",
    "Just use a pre-trained model as we do  \n",
    "Using pre-trained model from there: https://github.com/balancap/SSD-Tensorflow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "this_dir = os.getcwd()\n",
    "lib_path = os.path.join(this_dir, 'SSD-Tensorflow')\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "from nets import ssd_vgg_300, ssd_common, np_methods\n",
    "from preprocessing import ssd_vgg_preprocessing\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/jovyan/work/NetsLab/data/SSD-VGG/VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Input placeholder.\n",
    "net_shape = (300, 300)\n",
    "data_format = 'NHWC'\n",
    "img_input = tf.placeholder(tf.uint8, shape=(None, None, 3))\n",
    "# Evaluation pre-processing: resize to SSD net shape.\n",
    "image_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(\n",
    "    img_input, None, None, net_shape, data_format, resize=ssd_vgg_preprocessing.Resize.WARP_RESIZE)\n",
    "image_4d = tf.expand_dims(image_pre, 0)\n",
    "\n",
    "# Define the SSD model.\n",
    "reuse = True if 'ssd_net' in locals() else None\n",
    "ssd_net = ssd_vgg_300.SSDNet()\n",
    "with slim.arg_scope(ssd_net.arg_scope(data_format=data_format)):\n",
    "    predictions, localisations, _, _ = ssd_net.net(image_4d, is_training=False, reuse=reuse)\n",
    "\n",
    "# Restore SSD model.\n",
    "ckpt_filename = this_dir + '/data/SSD-VGG/VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt'\n",
    "isess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(isess, ckpt_filename)\n",
    "\n",
    "# SSD default anchor boxes.\n",
    "ssd_anchors = ssd_net.anchors(net_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main image processing routine.\n",
    "def process_image(img, select_threshold=0.5, nms_threshold=.45, net_shape=(300, 300)):\n",
    "    # Run SSD network.\n",
    "    rimg, rpredictions, rlocalisations, rbbox_img = isess.run([image_4d, predictions, localisations, bbox_img],\n",
    "                                                              feed_dict={img_input: img})\n",
    "    \n",
    "    # Get classes and bboxes from the net outputs.\n",
    "    rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(\n",
    "            rpredictions, rlocalisations, ssd_anchors,\n",
    "            select_threshold=select_threshold, img_shape=net_shape, num_classes=21, decode=True)\n",
    "    \n",
    "    rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)\n",
    "    rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=400)\n",
    "    rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)\n",
    "    # Resize bboxes to original image shape. Note: useless for Resize.WARP!\n",
    "    rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)\n",
    "    return rclasses, rscores, rbboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] [ 0.99764711] [[ 0.00254753  0.          0.9482094   0.9998045 ]]\n"
     ]
    }
   ],
   "source": [
    "# Test on some demo image and visualize output.\n",
    "path = this_dir + '/data/images/'\n",
    "output_path = this_dir + '/data/images_out/ssd_vgg/'\n",
    "image_names = sorted(os.listdir(path))\n",
    "\n",
    "############# Performance measurements block ###########\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# perf_image = this_dir + \"/data/images/\" + \"000542.jpg\"\n",
    "# times = list()\n",
    "# img = mpimg.imread(perf_image)\n",
    "# for i in tqdm(range(100)):\n",
    "#     timestart = time.time()\n",
    "#     isess.run([image_4d, predictions, localisations, bbox_img], feed_dict={img_input: img})\n",
    "#     times.append(time.time() - timestart)\n",
    "# print(times)    \n",
    "# performance = np.array(times)\n",
    "# print(\"Mean:\", np.mean(performance))\n",
    "# print(\"Median:\", np.median(performance))\n",
    "# print(\"Q25:\", np.percentile(performance, 25))\n",
    "# print(\"Q75:\", np.percentile(performance, 75))\n",
    "##############################################################\n",
    "\n",
    "########### Accuracy block ####################\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "ann_file = this_dir + '/data/annotations/instances_val2017.json'\n",
    "test_images_path = this_dir + '/data/test_data/'\n",
    "test_image_names = sorted(os.listdir(path))\n",
    "cat_id_to_real_id = \\\n",
    "    {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16,\n",
    "     18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30,\n",
    "     35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44,\n",
    "     50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58,\n",
    "     64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72,\n",
    "     82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n",
    "    \n",
    "def parse_annotation(ann_file, img_dir, labels=[]):\n",
    "    all_imgs = dict()\n",
    "    seen_labels = set()\n",
    "    with open(ann_file) as f:\n",
    "        inst = json.load(f)\n",
    "    boxes = {image['id']:[] for image in inst['images']}\n",
    "    cats = inst[\"categories\"]\n",
    "    for ann in inst['annotations']:\n",
    "        obj = {}\n",
    "        real_id = cat_id_to_real_id[ann['category_id']]\n",
    "        obj['name'] = cats[real_id-1][\"name\"]\n",
    "        seen_labels.add(obj['name'])\n",
    "        obj['xmin'] = int(round(ann['bbox'][0]))\n",
    "        obj['ymin'] = int(round(ann['bbox'][1]))\n",
    "        obj['xmax'] = int(round(ann['bbox'][0] + ann['bbox'][2]))\n",
    "        obj['ymax'] = int(round(ann['bbox'][1] + ann['bbox'][3]))\n",
    "        boxes[ann['image_id']].append(obj)\n",
    "    for image in inst['images']:\n",
    "        all_imgs[os.path.join(img_dir,image['file_name'])] =  boxes[image['id']]\n",
    "    return all_imgs, seen_labels\n",
    "\n",
    "imgs, labels = parse_annotation(ann_file, test_images_path)\n",
    "\n",
    "aaaa = test_images_path + test_image_names[0]\n",
    "img = mpimg.imread(perf_image)\n",
    "rclasses, rscores, rbboxes = process_image(img)\n",
    "\n",
    "print(rclasses, rscores, rbboxes)\n",
    "\n",
    "###############################################\n",
    "\n",
    "\n",
    "\n",
    "# for image_name in image_names:\n",
    "#     print(image_name)\n",
    "#     img = mpimg.imread(path + image_name)\n",
    "#     rclasses, rscores, rbboxes =  process_image(img)\n",
    "#     visualization.plt_bboxes(img, rclasses, rscores, rbboxes, output_path+image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2.01650357723\n",
      "Median: 2.16040587425\n",
      "Q25: 1.18544638157\n",
      "Q75: 2.57627117634\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
